{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jlaing100/RecipeProject/blob/main/Recipe_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_2zjauyHN8_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#csv.open('/content/drive/MyDrive/Recipe/archive/RecipeNLG_dataset.csv')\n",
        "import pandas as pd\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Recipe/archive/RecipeNLG_dataset.csv\") # dataset borrowed from kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data\n"
      ],
      "metadata": {
        "id": "T64at-x9ZX7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4L6Yx5vwg4C"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "#model = api.load('word2vec-google-news-300')  # A 300-dimensional model trained on Google News.\n",
        "word2vec_model = api.load(\"glove-wiki-gigaword-200\")\n",
        "len(word2vec_model.index_to_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDL20TXtIfNx"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "from transformers import pipeline\n",
        "token_classifier = pipeline(\"ner\", model=\"davanstrien/distilbert-base-cased_fine_tuned_food_ner\", tokenizer=\"davanstrien/distilbert-base-cased_fine_tuned_food_ner\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKxeD1Db3tIw"
      },
      "source": [
        "## **`MAIN`**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model.most_similar(\"wine\")"
      ],
      "metadata": {
        "id": "IJUoWrf_i29e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70hcQqP1fwbk"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import sys\n",
        "from transformers import pipeline\n",
        "sentiment_classifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=True)\n",
        "\n",
        "most_similar = word2vec_model.most_similar\n",
        "sub_cue = \"I don't have\"\n",
        "\n",
        "def substitution(ingredient):\n",
        "\n",
        "\n",
        "    substitutes_with_scores = most_similar(ingredient, topn=5)\n",
        "\n",
        "\n",
        "    substitutes = [substitute[0] for substitute in substitutes_with_scores]\n",
        "\n",
        "    print(f\"Looking for substitutes for: {ingredient}\")\n",
        "\n",
        "    if substitutes:\n",
        "        print(f\"Possible substitutes for {ingredient}: {', '.join(substitutes)}\")\n",
        "    else:\n",
        "        print(f\"No substitutes found for {ingredient}\")\n",
        "    sys.exit()\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "  input_string = input(\"How can I help?\")\n",
        "\n",
        "  # split on 'and', 'or', and punctuation\n",
        "  pattern = r'[.,!?]|\\b(and|or|but)\\b'\n",
        "\n",
        "  #use the re.split() function to split the string based on the pattern\n",
        "  substrings = re.split(pattern, input_string)\n",
        "\n",
        "  #filter out any empty or whitespace-only substrings\n",
        "  substrings = [substring.strip() if substring is not None else \"\" for substring in substrings]\n",
        "\n",
        "\n",
        "  substrings = [substring for substring in substrings if substring]\n",
        "\n",
        "\n",
        "  recipe_count = 0\n",
        "  max_recipes = 10\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  unwanted_list = []\n",
        "  unwanted_ingredients = []\n",
        "  for substring in substrings:\n",
        "      sub_cue = \"I don't have \"\n",
        "      result = sentiment_classifier(substring)\n",
        "\n",
        "\n",
        "      ingredient = substring.split(sub_cue)[-1].strip()\n",
        "      substitution(ingredient)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  for substring in substrings:\n",
        "      result = sentiment_classifier(substring)\n",
        "      anger_score = result[0][0]['score']\n",
        "      disgust_score = result[0][1]['score']\n",
        "      unwanted_ingredients = token_classifier(substring)\n",
        "      user_input = substring\n",
        "\n",
        "          #identify the ingredients in the string the user used unfavorably\n",
        "      if (anger_score >= 0.5) or (disgust_score >= 0.5):\n",
        "        ingredients = token_classifier(substring)\n",
        "\n",
        "        #negative_ingredients = [entity['word'] for entity in ingredients if entity['entity'] == 'U-FOOD']\n",
        "\n",
        "        unwanted_list = [item['word'] for item in ingredients]\n",
        "\n",
        "        for item in unwanted_list:\n",
        "          print(\"We'll be sure to avoid\", item )\n",
        "\n",
        "\n",
        "        #call substitution function for each unwanted ingredient\n",
        "\n",
        "        #iterate over each entity in output_data\n",
        "        \"\"\"\n",
        "        for entity in negative_ingredients:\n",
        "            # Check if the 'entity' key is 'U-FOOD'\n",
        "                # Append the 'word' to the food_tokens_list\n",
        "            if negative_ingredients:\n",
        "              unwanted_ingredients.append(negative_ingredients)\n",
        "        \"\"\"\n",
        "\n",
        "  for substring in substrings:\n",
        "      result = sentiment_classifier(substring)\n",
        "      anger_score = result[0][0]['score']\n",
        "      disgust_score = result[0][1]['score']\n",
        "      joy_score = result[0][3]['score']\n",
        "      neutral_score = result[0][4]['score']\n",
        "      recipe_count = 0\n",
        "\n",
        "      if (joy_score >= 0.5) or (neutral_score >= 0.5):\n",
        "\n",
        "          #pass the substring to the token identification model\n",
        "          user_input = substring\n",
        "\n",
        "          ingredients = token_classifier(user_input)\n",
        "\n",
        "\n",
        "          desired_ingredients = [entity['word'] for entity in ingredients if entity['entity'] == 'U-FOOD']\n",
        "          if desired_ingredients:\n",
        "            print(f\"Here are some great recipes with {desired_ingredients[0]}!\\n\")\n",
        "\n",
        "\n",
        "          matching_recipes = data[data['ingredients'].apply(lambda x: all(item in x for item in desired_ingredients) and not any(item in x for item in unwanted_list))]\n",
        "\n",
        "          for index, row in matching_recipes.iterrows():\n",
        "\n",
        "              matched_ner = [ingredient for ingredient in desired_ingredients if ingredient in row['NER'] and ingredient not in unwanted_list]\n",
        "              #print matched recipes\n",
        "              print(f\"Recipe: {row['title']}\")\n",
        "              print(\"Matched Ingredients (from NER):\")\n",
        "              print(', '.join(matched_ner))\n",
        "              print(\"Directions:\")\n",
        "              print(row['directions'])\n",
        "              print(\"\\n\")\n",
        "\n",
        "              recipe_count += 1\n",
        "\n",
        "              if recipe_count >= max_recipes:\n",
        "                 break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-vu7QgpOpdL"
      },
      "outputs": [],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eDE0IJwwxzW"
      },
      "outputs": [],
      "source": [
        "#content\n",
        "for index, word in enumerate(word2vec_model.index_to_key):\n",
        "    if index < 10:\n",
        "        print(f\"word #{index}/{len(word2vec_model.index_to_key)} is {word}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evAOespN36KE"
      },
      "source": [
        "SUBSTITUTION **FUNCTION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVCYfw8WUowR"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def extract_and_classify_text_after_phrase(phrase, text, token_classifier):\n",
        "    pattern = re.compile(rf\"{re.escape(phrase)}(.*?)\\.\")\n",
        "    match = re.search(pattern, text)\n",
        "\n",
        "    if match:\n",
        "        result = match.group(1).strip()\n",
        "        result_token = token_classifier(result)\n",
        "        no_ingredients = [entity['word'] for entity in result_token if entity['entity'] == 'U-FOOD']\n",
        "        return no_ingredients\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "text_example = \"I don't have any potato and chocolate.\"\n",
        "search_phrase = \"I don't have\"\n",
        "\n",
        "result = extract_and_classify_text_after_phrase(search_phrase, text_example, token_classifier)\n",
        "\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "id": "1Meqx1f-ayGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "\n",
        "def main():\n",
        "    st.title(\"Recipe Substitution Finder\")\n",
        "    user_input = st.text_input(\"Enter the ingredient you don't have or want to replace:\")\n",
        "    submit_button = st.button('Find Recipes')\n",
        "\n",
        "    if submit_button and user_input:\n",
        "        substituted_recipes = substitution(user_input)\n",
        "        if substituted_recipes:\n",
        "            st.write(\"Here are some recipe suggestions:\")\n",
        "            for recipe in substituted_recipes:\n",
        "                st.write(recipe)\n",
        "        else:\n",
        "            st.write(\"No recipes found for the given ingredient.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "h7fDtXSnazqJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a746a0c4-afa0-4a83-9c73-630eda4555c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-12-08 21:51:18.796 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py Recipe.py\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acFtoR5fahDq",
        "outputId": "6eb3e10b-1ad8-445b-fe3d-93f5e45fdf8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.233.207.234:8501\u001b[0m\n",
            "\u001b[0m\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}